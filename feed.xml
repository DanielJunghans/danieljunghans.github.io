<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://danieljunghans.github.io/al-folio/feed.xml" rel="self" type="application/atom+xml" /><link href="https://danieljunghans.github.io/al-folio/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-11-07T01:17:08+00:00</updated><id>https://danieljunghans.github.io/al-folio/feed.xml</id><title type="html">blank</title><subtitle>My name is Daniel Junghans and I am a Financial Institution Examiner for Michigan&apos;s Department of Insurance and Financial Services. I work within the Office of Banking to protect depositors, creditors, and shareholders. I am a knowledgeable young finance professional eager to leverage my understanding of finance to generate creative solutions. 
</subtitle><entry><title type="html">Ongoing Liquidity Trends for Michigan Banks</title><link href="https://danieljunghans.github.io/al-folio/blog/2023/LiquidityTrends/" rel="alternate" type="text/html" title="Ongoing Liquidity Trends for Michigan Banks" /><published>2023-11-04T08:58:00+00:00</published><updated>2023-11-04T08:58:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2023/LiquidityTrends</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2023/LiquidityTrends/"><![CDATA[<p align="justify"><b>DISCLAIMER:</b> This is a personal blog.  Any views or opinions expressed in this blog do not represent the official policy or position of the Michigan Department of Insurance and Financial Services.   </p>

<p style="text-align: left;"><font size="+3"><b>Introduction</b></font></p>

<p align="justify">
The goal of this article will be to identify ongoing liquidity trends for Michigan Banks.  In my last article titled Liquidity Determinants of Michigan Banks, I investigated bank-specific factors that affect the level of liquid assets held by state-chartered Michigan Banks.  This analysis used balanced panel data for 59 state-chartered commercial banks from December 31st, 2019, until June 30th, 2023.  Fixed effect regression analysis concluded that profitability, deposits, capital adequacy, interest margin, and asset quality have a significant effect on the level of liquid assets.  Using the same dataset, this article will further analyze these bank-specific factors.  Small, medium, and large Michigan banks with assets totaling less than $300 million, $300 million to $1 billion, and over $1 billion, respectively, will be analyzed independently. 
</p>

<p style="text-align: left;"><font size="+3"><b>Liquidity Trends Over the Past Year</b></font></p>

<p align="justify">
As of June 30th, 2023, the average level of liquid assets held by Michigan banks has declined over the past four quarters.  Average liquid assets have declined by 9.23, 33.02, and 37.92 percent for small, medium, and large Michigan banks, respectively.  The decline in average liquid assets for Michigan banks has been driven by declining interest-bearing bank balances.  Declining interest-bearing bank balances as well as an increase in wholesale funds have been utilized to grow average total loans. 
</p>

<p align="justify">
Average total loans for Michigan banks have grown over the past year. The level of average total loans has grown by $10.5, $64.4, and $115.9 million for small, medium, and large banks, respectively.  Loan growth for Michigan banks has been driven by the growth of residential and commercial real estate.  Average residential real estate loans for Michigan banks have grown by 21 percent while commercial real estate loans have grown by 13 percent year over year.  As a result of loan growth and the rising interest rate environment, average loan &amp; lease income has grown by 32, 43, and 42 percent for small, medium, and large Michigan Banks. 
</p>

<p align="justify">
In my prior article, net interest margin was found to have a significant negative effect on the level of liquidity for Michigan banks.  This is likely due to an increase in opportunity cost for banks.  When the yield on illiquid assets increases, the opportunity cost of holding lower yielding liquid assets grows.  Average net interest margin has increased by 46, 47, and 45 basis points for small, medium, and large Michigan banks, respectively.  The growth of average net interest margin for Michigan banks has been largely due to the growth of loans and loan income. 
</p>

<p align="justify">
Another key finding from my prior work was that deposits have a significant positive effect on the level of liquid assets held by Michigan banks.  It is likely that Michigan banks hold more liquid assets as total deposits increase in order to mitigate risk stemming from an increase in cash demanded from depositors.  Average total deposits over total assets have declined by 158, 222, and 270 basis points for small, medium, and large banks, respectively.  Based on the findings from my prior study, the declining level of deposits relative to total assets has likely contributed to the decline of average liquid assets.  The decline in average total deposits over total assets has been significantly impacted by an increase in the usage of wholesale funding. 
</p>

<p align="justify">
As the level of deposits relative to total assets has declined, the average level of wholesale funds held by Michigan banks has increased.  The average level of wholesale funds for small, medium, and large Michigan banks has increased by $2.7, $34.6, and $223 million, respectively.  The growth of average wholesale funds for small banks has been driven by the average level of FHLB borrowings maturing under 1 year growing $962,500.  Increasing average wholesale funds for medium and large banks has been driven by average brokered deposits which have increased by $19,265,320 and $127,540,900, respectively.  
</p>

<p align="justify">
The Tier 1 Leverage ratio has a negative effect on the level of liquid assets according to my study on the determinants of liquid assets.  Michigan banks with higher Tier 1 Leverage ratios may hold fewer liquid assets due to the increased capacity for capital to absorb losses mitigating some liquidity risk.  An additional explanation could be that Michigan banks with a greater level of higher yielding illiquid assets generate more earnings which augment capital.  The average Tier 1 Leverage ratio for small, medium, and large Michigan banks have risen 52, 48, and 56 basis points, respectively.  Increasing average capitalization for Michigan banks has likely contributed to the decline in average liquid assets according to the results of my prior study. 
</p>

<p align="justify">
An additional finding from my article on liquidity determinants was that liquidity is positively affected by the level of nonaccrual loans.  Michigan banks with higher levels of nonaccrual loans may hold higher levels of liquidity in order to mitigate the risk of potential loan losses.  Over the prior four quarters, the average level of nonaccrual loans over total loans has grown by 4 and 3 basis points for small and large Michigan banks, respectively.  Average nonaccrual loans over total loans for Medium sized Michigan banks have declined by 19 basis points over the past year.  These changes in the level of nonaccrual loans are marginal and have likely had a small impact on the level of liquidity. 
</p>

<p align="justify">
Lastly, my prior article found that return on assets has a positive effect on liquidity for Michigan banks.  Michigan banks with high return on assets could have more risk on their balance sheet requiring a larger liquidity buffer to mitigate risk.  Average return on assets for small and large Michigan banks have increased 31 and 15 basis points, respectively.  Average return on assets for medium sized Michigan banks have remained stable at 1.06 percent.  Although my last article found that return on assets has a positive effect on liquidity, average levels of liquidity have fallen as average return on assets for Michigan banks have increased.  This is likely due to the rising average net interest margin having a more significant impact on liquidity.  In the second fixed effect regression model in my last study, liquidity declined by 8.45 percent for every 1 percent increase in net interest margin while liquidity only increased by 2.73 percent for every 1 percent increase in return on assets.
</p>

<p style="text-align: left;"><font size="+3"><b>Banks With Less Than $300 Million in Total Assets
</b></font></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/under300.png" />
</div>
<p>Figure 1. Liquidity Trends for Michigan Banks with Less than $300 Million in Total Assets
<br /></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/TLunder300.png" />
</div>
<p>Figure 2.  Average Loans for Michigan Banks with Total Assets Less Than $300 million
<br /></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/WFunder300.png" />
</div>
<p>Figure 3.  Average Wholesale Funding for Michigan Banks with Total Assets Less Than $300 Million
<br /></p>

<p style="text-align: left;"><font size="+3"><b>Banks with Total Assets Between $300 Million and $1 Billion</b></font></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/300_1b.png" />
</div>
<p>Figure 4. Liquidity Trends for Banks with Total Assets Between $300 Million and $1 Billion
<br /></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/TL300_1b.png" />
</div>
<p>Figure 5.  Average Loans for Michigan Banks with Total Assets Between $300 million and $1 Billion
<br /></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/WF300_1b.png" />
</div>
<p>Figure 6.  Average Wholesale Funding for Michigan Banks with Total Assets Between $300 Million and $1 Billion
<br /></p>

<p style="text-align: left;"><font size="+3"><b>Banks with Total Assets Over $1 Billion</b></font></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/over1b.png" />
</div>
<p>Figure 7. Liquidity Trends for Banks with Total Assets Over $1 Billion
<br /></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/TLOver_1b.png" />
</div>
<p>Figure 8.  Average Loans for Michigan Banks with Total Assets Over $1 Billion
<br /></p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/WFover_1b.png" />
</div>
<p>Figure 9.  Average Wholesale Funding for Michigan Banks with Total Assets Over $1 Billion
<br /></p>]]></content><author><name></name></author><category term="Liquidity" /><category term="Banking" /><category term="Michigan" /><summary type="html"><![CDATA[DISCLAIMER: This is a personal blog. Any views or opinions expressed in this blog do not represent the official policy or position of the Michigan Department of Insurance and Financial Services. Introduction The goal of this article will be to identify ongoing liquidity trends for Michigan Banks. In my last article titled Liquidity Determinants of Michigan Banks, I investigated bank-specific factors that affect the level of liquid assets held by state-chartered Michigan Banks. This analysis used balanced panel data for 59 state-chartered commercial banks from December 31st, 2019, until June 30th, 2023. Fixed effect regression analysis concluded that profitability, deposits, capital adequacy, interest margin, and asset quality have a significant effect on the level of liquid assets. Using the same dataset, this article will further analyze these bank-specific factors. Small, medium, and large Michigan banks with assets totaling less than $300 million, $300 million to $1 billion, and over $1 billion, respectively, will be analyzed independently. Liquidity Trends Over the Past Year As of June 30th, 2023, the average level of liquid assets held by Michigan banks has declined over the past four quarters. Average liquid assets have declined by 9.23, 33.02, and 37.92 percent for small, medium, and large Michigan banks, respectively. The decline in average liquid assets for Michigan banks has been driven by declining interest-bearing bank balances. Declining interest-bearing bank balances as well as an increase in wholesale funds have been utilized to grow average total loans. Average total loans for Michigan banks have grown over the past year. The level of average total loans has grown by $10.5, $64.4, and $115.9 million for small, medium, and large banks, respectively. Loan growth for Michigan banks has been driven by the growth of residential and commercial real estate. Average residential real estate loans for Michigan banks have grown by 21 percent while commercial real estate loans have grown by 13 percent year over year. As a result of loan growth and the rising interest rate environment, average loan &amp; lease income has grown by 32, 43, and 42 percent for small, medium, and large Michigan Banks. In my prior article, net interest margin was found to have a significant negative effect on the level of liquidity for Michigan banks. This is likely due to an increase in opportunity cost for banks. When the yield on illiquid assets increases, the opportunity cost of holding lower yielding liquid assets grows. Average net interest margin has increased by 46, 47, and 45 basis points for small, medium, and large Michigan banks, respectively. The growth of average net interest margin for Michigan banks has been largely due to the growth of loans and loan income. Another key finding from my prior work was that deposits have a significant positive effect on the level of liquid assets held by Michigan banks. It is likely that Michigan banks hold more liquid assets as total deposits increase in order to mitigate risk stemming from an increase in cash demanded from depositors. Average total deposits over total assets have declined by 158, 222, and 270 basis points for small, medium, and large banks, respectively. Based on the findings from my prior study, the declining level of deposits relative to total assets has likely contributed to the decline of average liquid assets. The decline in average total deposits over total assets has been significantly impacted by an increase in the usage of wholesale funding. As the level of deposits relative to total assets has declined, the average level of wholesale funds held by Michigan banks has increased. The average level of wholesale funds for small, medium, and large Michigan banks has increased by $2.7, $34.6, and $223 million, respectively. The growth of average wholesale funds for small banks has been driven by the average level of FHLB borrowings maturing under 1 year growing $962,500. Increasing average wholesale funds for medium and large banks has been driven by average brokered deposits which have increased by $19,265,320 and $127,540,900, respectively. The Tier 1 Leverage ratio has a negative effect on the level of liquid assets according to my study on the determinants of liquid assets. Michigan banks with higher Tier 1 Leverage ratios may hold fewer liquid assets due to the increased capacity for capital to absorb losses mitigating some liquidity risk. An additional explanation could be that Michigan banks with a greater level of higher yielding illiquid assets generate more earnings which augment capital. The average Tier 1 Leverage ratio for small, medium, and large Michigan banks have risen 52, 48, and 56 basis points, respectively. Increasing average capitalization for Michigan banks has likely contributed to the decline in average liquid assets according to the results of my prior study. An additional finding from my article on liquidity determinants was that liquidity is positively affected by the level of nonaccrual loans. Michigan banks with higher levels of nonaccrual loans may hold higher levels of liquidity in order to mitigate the risk of potential loan losses. Over the prior four quarters, the average level of nonaccrual loans over total loans has grown by 4 and 3 basis points for small and large Michigan banks, respectively. Average nonaccrual loans over total loans for Medium sized Michigan banks have declined by 19 basis points over the past year. These changes in the level of nonaccrual loans are marginal and have likely had a small impact on the level of liquidity. Lastly, my prior article found that return on assets has a positive effect on liquidity for Michigan banks. Michigan banks with high return on assets could have more risk on their balance sheet requiring a larger liquidity buffer to mitigate risk. Average return on assets for small and large Michigan banks have increased 31 and 15 basis points, respectively. Average return on assets for medium sized Michigan banks have remained stable at 1.06 percent. Although my last article found that return on assets has a positive effect on liquidity, average levels of liquidity have fallen as average return on assets for Michigan banks have increased. This is likely due to the rising average net interest margin having a more significant impact on liquidity. In the second fixed effect regression model in my last study, liquidity declined by 8.45 percent for every 1 percent increase in net interest margin while liquidity only increased by 2.73 percent for every 1 percent increase in return on assets. Banks With Less Than $300 Million in Total Assets Figure 1. Liquidity Trends for Michigan Banks with Less than $300 Million in Total Assets Figure 2. Average Loans for Michigan Banks with Total Assets Less Than $300 million Figure 3. Average Wholesale Funding for Michigan Banks with Total Assets Less Than $300 Million Banks with Total Assets Between $300 Million and $1 Billion Figure 4. Liquidity Trends for Banks with Total Assets Between $300 Million and $1 Billion Figure 5. Average Loans for Michigan Banks with Total Assets Between $300 million and $1 Billion Figure 6. Average Wholesale Funding for Michigan Banks with Total Assets Between $300 Million and $1 Billion Banks with Total Assets Over $1 Billion Figure 7. Liquidity Trends for Banks with Total Assets Over $1 Billion Figure 8. Average Loans for Michigan Banks with Total Assets Over $1 Billion Figure 9. Average Wholesale Funding for Michigan Banks with Total Assets Over $1 Billion]]></summary></entry><entry><title type="html">Liquidity Determinants of Michigan Banks</title><link href="https://danieljunghans.github.io/al-folio/blog/2023/Liquidity/" rel="alternate" type="text/html" title="Liquidity Determinants of Michigan Banks" /><published>2023-10-14T08:58:00+00:00</published><updated>2023-10-14T08:58:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2023/Liquidity</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2023/Liquidity/"><![CDATA[<p align="justify"><b>DISCLAIMER:</b> This is a personal blog.  Any views or opinions expressed in this blog do not represent the official policy or position of the Michigan Department of Insurance and Financial Services.   </p>

<p style="text-align: left;"><font size="+3"><b>Introduction</b></font></p>

<p align="justify">
This article investigates bank-specific factors that determine the level of liquid assets held by state chartered commercial banks in Michigan.  Bank-specific factors include bank size, profitability, capital adequacy, deposits, and asset quality.  This study consists of balanced panel data of 59 state chartered commercial banks from December 31st, 2019, until June 30th, 2023.   

</p>
<p align="justify">
Under the fractional reserve banking system, banks only hold a fraction of their deposit liabilities in liquid assets to meet anticipated liquidity needs.  Bank management is responsible for determining the appropriate level of liquid assets required to meet the demands of depositors and creditors while allowing for the greatest profitability.  Developing a better understanding of the determinants of liquid assets will give key insights into ongoing liquidity trends.  

</p>

<p style="text-align: left;"><font size="+3"><b>Literature Review</b></font></p>

<p align="justify">
In 1999, an analysis of the Mexican banking system found that capital had a significant positive effect on liquid assets while total asset size (in relation to the total assets of the banking system) had no statistically significant effect on any measure of liquid assets.  Additionally, this study theorized that banks with relatively more demand deposits possess more liquid assets but found that the level of demand deposits relative to total assets had a significant negative effect on securities and liquid assets but not the level of cash.  This finding could suggest that banks consider demand deposits to be a very stable liability, or the variance of deposit withdraws declines as deposits increase and the deposit population becomes more diverse (Alger &amp; Alger, 1999). 

</p>
<p align="justify">
The relationship between liquid assets and bank size was further examined in a 2015 study of 18 Tunisian banks.  This study used the natural logarithm of total assets as a measure of bank size and also found that bank size did not have a significant impact on bank liquidity.  Additional variables in this study included return on assets (ROA), return on equity (ROE), net interest margin (NIM), total loans, operating expenses, total deposits, financial expenses, equity, inflation rate, and growth rate of GDP.  ROA, ROE, capital, operating expenses, growth rate of GDP, and  inflation rate were found to have had a significant impact on bank liquidity.  Total loans, financial costs, and total deposits did not have a significant impact on bank liquidity (Moussa, 2015). 

</p>
<p align="justify">
Macroeconomic as well as bank-specific determinants of liquidity of Indian banks were studied by Singh and Sharma (2016).  Bank-specific determinants studied in this paper include bank size, profitability, cost of funding, capital adequacy, and deposits.  Macroeconomic determinants of liquidity included GDP, inflation, and unemployment.  This study found that bank size and GDP had a negative relationship with liquidity at a 5 percent confidence interval while profitability, deposits, inflation, and capital adequacy had a positive impact.  

</p>

<p style="text-align: left;"><font size="+3"><b>Specification of Variables</b></font></p>

<p align="justify">
The dependent variable in this article is liquidity which was measured through two liquidity ratios provided by Vodová (2011).  Fixed and random effect regression was performed for both measures of liquidity.  The first measure of liquidity is net loans as a fraction of total assets (Liq1).  The ratio Liq1 indicates the percentage of bank assets which are invested in illiquid loans.  The higher this ratio is, the fewer liquid assets a bank possesses.  The second measure of liquidity is liquid assets as a fraction of total assets (Liq2).  The ratio Liq2 indicates the percentage of bank assets which are liquid.  A high ratio may indicate an inefficiency since liquid assets yield less income than illiquid assets.  Liquid assets consist of interest as well as non-interest bearing bank balances,  securities sold under agreement to repurchase, federal funds sold, and total securities less pledged securities.

</p>

<p>Table 1
<br />
Summary of variables and expected relationships with the dependent variables.</p>

<table border="1" width="100%">
  <tr><th>Independent Variable</th><th>Proxy/Measurement</th><th>Notation</th><th>Expected Effect</th></tr>
  <tr><td>Bank Size</td><td>Natural log of total assets</td><td>SIZE</td>  <td>Negative</td></tr>
  <tr><td>Profitability</td><td>Return on assets</td><td>ROA</td>  <td>Negative</td></tr>
  <tr><td>Deposits</td><td>Deposits over total assets</td><td>DEP</td>  <td>Positive</td></tr>
  <tr><td>Capital Adequacy</td><td>Tier 1 leverage ratio</td><td>CAP</td>  <td>Positive</td></tr>
  <tr><td>Interest Margin</td><td>Net interest margin</td><td>NIM</td>  <td>Negative</td></tr>
  <tr><td>Asset Quality</td><td>Nonaccrual loans over total loans</td><td>IMLOAN</td>  <td>Negative</td></tr>
</table>

<p><br /></p>

<ul>

<li><p align="justify"><b>Bank Size (SIZE)</b>.  The size of banks (SIZE) has been measured using the natural logarithm of total assets.  Sing et al. (2016) and El-Chaarani (2019) stated that bank size has a significant negative effect on liquidity.  Smaller banks rely on a buffer of liquid assets while larger banks rely on credit instruments and the inter-bank market which is in accordance with the “too big to fail” hypothesis.  
</p></li>
<li><p align="justify"><b>Profitability (ROA)</b>.  Return on assets is the proxy of profitability in this article.  Return on Assets (TTM) measures the net income over the prior four quarters over average total assets.  Moussa (2015) found that banks with higher return on assets had lower levels of liquidity.  Banks with less liquid assets have a higher return on assets due to liquid assets generally yielding less income than illiquid assets.
</p></li>
<li><p align="justify"><b>Deposits (DEP)</b>.  Deposits (DEP) have been calculated as total deposits over total assets.  Sing et al. (2016) stated that the level of deposits has a positive effect on liquidity.  Deposits are the major source of funds for banks and these institutions must maintain adequate levels of liquidity to meet customer demand.  </p></li>
<li><p align="justify"><b>Capital Adequacy (CAP)</b>.  The tier 1 leverage ratio is the proxy of capital adequacy in this article. Sing et al. (2016) and Alger et al. (1999) found that capital adequacy had a positive influence on liquidity as a whole.  Alger et al. (1999) suggests that banks invest more in liquid assets as a precaution when more capital is at stake.
 </p></li>
<li><p align="justify"><b>Interest Margin (NIM)</b>.  Net interest margin is the measure of interest income less interest expenses over average earning assets.  According to finance theory, interest margin should be negatively correlated with liquidity.  Vodová (2013) found that liquidity is negatively affected by interest margin. 
 </p></li>
<li><p align="justify"><b>Asset Quality (IMLOAN)</b>.  Total nonaccrual loans over total loans is the proxy of asset quality in this article.  Munteanu (2012) stated that the level of impaired loans has a significant negative effect on liquidity.  A higher level of nonaccrual loans may lead to increased lending activity to make up for losses thus decreasing the level of liquid assets. </p></li>

</ul>

<p style="text-align: left;"><font size="+3"><b>Model Specification</b></font></p>

<p>Two models will be estimated:</p>

\[Liq1_{it} = \alpha_{it}+β_{1}SIZE_{it}+β_{2}ROA_{it}+β_{3}DEP_{it}+β_{4}CAP_{it}+β_{5}IMLOAN_{it}+ ε_{it}\]

\[Liq2_{it} = \alpha_{it}+β_{1}SIZE_{it}+β_{2}ROA_{it}+β_{3}DEP_{it}+β_{4}CAP_{it}+β_{5}IMLOAN_{it}+ ε_{it}\]

<p align="justify">
β1, β2 , β3, β4, and β5 are the coefficients of the determinant variables and ε is the error term.  The panel data was constructed with indices “i” and “t” representing individual banks and time, respectively.  The data is comprised of 59 banks from December 31st, 2019, until June 30th, 2023.  The total number of observations is 885.  
</p>

<p style="text-align: left;"><font size="+3"><b>Descriptive Statistics</b></font></p>

<p align="justify">
Table 2 highlights the descriptive statistics for state chartered Michigan banks.  Based on the findings below, net loans on average represented 59 percent of total assets (Liq1) for Michigan banks.  Additionally, liquid assets constituted 33 percent of total assets (Liq2) on average. 
</p>

<p>Table 2 
<br />
Descriptive Statistics (Sample 12/31/2019 - 06/30/2023)</p>
<div class="img">
    <img class="col three" src="/al-folio/assets/img/summary_stats.png" />
</div>

<p><br /></p>

<p style="text-align: left;"><font size="+3"><b>Fixed Effect and Random Effect Regression</b></font></p>

<p align="justify">
Fixed effect and random effect regressions were run and a Hausman test was carried out to choose the appropriate model.  The Hausman Test for both measures of liquidity resulted in a p-value less than 0.05, confirming that the fixed effect regression was the preferred model for Liq1 and Liq2.  Results are shown in Table 3. 

<br />
<br />

The fixed effect models concluded that DEP, IMLOAN, CAP, NIM, and ROA significantly impacted the first measure of liquidity (Liq1) as well as the second measure (Liq2).  The impact of DEP, IMLOAN, and ROA on Liq1 (net loans / total assets) was negative whereas the impact of CAP and NIM was positive.  However, the impact of DEP, IMLOAN, and ROA on Liq2 (liquid assets / total assets) was positive while the impact of CAP and NIM was negative.  SIZE was found to have an insignificant effect on either liquidity measure (Table 4).   
</p>

<p>Table 3
<br />
Hausman Test</p>

<table border="1" width="100%">
  <tr><th>Model</th><th>Chi-Sq. Statistic</th><th>Prob.</th></tr>
  <tr><td>Liq1 Model</td><td>33.965</td><td>6.833e-06</td></tr>
  <tr><td>Liq2 Model</td><td>56.274</td><td>2.562e-10</td></tr>
  
</table>

<p><br /></p>

<p>Table 4
<br />
Regression Analysis</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/regressionresults.png" />
</div>

<p><br /></p>

<p style="text-align: left;"><font size="+3"><b>Conclusion</b></font></p>

<p align="justify">
The aim of this article was to identify bank-specific determinants of liquidity for state chartered Michigan Banks.  Panel data regression analysis has been used for two measures of liquidity.  Bank-specific determinants analyzed include bank size, total deposits, asset quality, capital adequacy, interest margin, and profitability.  The findings of this study afford key insights into ongoing liquidity trends and risk. 
</p>

<p align="justify">
Liquidity is positively affected by the level of total deposits.  In other words, the greater the level of total deposits, the higher the level of liquidity banks hold.  Similar results were found in a study by Sing et al. (2016).  However, a study by Alger et al. (1999) found a negative relationship between deposits and bank liquidity.  
</p>

<p align="justify">
Another major finding of this study is that the level of nonaccrual loans has a positive effect on liquidity.  Thus, Michigan banks hold more liquid assets the higher the level of nonaccrual loans.  This finding contradicts studies by Munteanu (2012) and El-Chaarani (2019) that have found that nonperforming loans have a negative impact on bank liquidity. 
</p>

<p align="justify">
Capital adequacy has a significant negative effect on liquidity.  This study contradicts the findings of Sing et al. (2016) and Alger et al. (1999) but is in line with a study conducted by Moussa (2015).  The reasons behind this discrepancy is a subject for future study.  
</p>

<p align="justify">
In line with the findings of Vodová (2013), net interest margin has a negative effect on bank liquidity.  If net interest margin increases by 1 percent, liquidity declines by 8.25 percent and 8.45 percent in the Liq1 and Liq2 models, respectively.  Vodová (2013) concluded that higher interest margin leads to increased lending activity thus reducing bank liquidity.  
</p>

<p align="justify">
The level of liquid assets held by Michigan banks is positively affected by bank profitability.  Similar results were reported by Vodová (2013) and Sing et al. (2016).  Higher profitability could indicate that more risk is present on the balance sheet which could require a higher liquidity buffer to mitigate risk. 
</p>

<p align="justify">
Bank size has no statistically significant relationship with the liquidity of Michigan state chartered banks.  This may be due to the size of the financial institutions included in this article.  Studies by Sing et al. (2016) and El-Chaarani (2019) found that smaller banks held a buffer of liquid assets while larger banks held less liquid assets and relied on credit instruments and the inter-bank market.  The banks included in this study may be too small to rely on credit instruments and the inter-bank market to meet liquidity needs. 
</p>

<p style="text-align: left;"><font size="+3"><b>References</b></font></p>
<p align="justify">
Alger, G., &amp; Alger, I. (1999). Liquid assets in banks: Theory and practice. GREMAQ, Universite des Sciences Sociales.
</p>

<p align="justify">
El-Chaarani, H. (2019). Determinants of bank liquidity in the Middle East region. El-CHAARANI H.,(2019), Determinants of Bank Liquidity in the Middle East Region, International Review of Management and Marketing, 9(2).
</p>

<p align="justify">
Moussa, M. A. B. (2015). The determinants of bank liquidity: Case of Tunisia. International journal of economics and financial issues, 5(1), 249-259.
</p>

<p align="justify">
Munteanu, I. (2012). Bank liquidity and its determinants in Romania. Procedia Economics and Finance, 3, 993-998.
</p>

<p align="justify">
Singh, A., &amp; Sharma, A. K. (2016). An empirical analysis of macroeconomic and bank-specific factors affecting liquidity of Indian banks. Future Business Journal, 2(1), 40-53.
</p>

<p align="justify">
Vodová, P. (2011). Liquidity of Czech commercial banks and its determinants. International Journal of mathematical models and methods in applied sciences, 5(6), 1060-1067.
</p>

<p align="justify">
Vodová, P. (2013). Determinants of commercial bank liquidity in Hungary. Finansowy Kwartalnik Internetowy e-Finanse, 9(3), 64-71.
</p>]]></content><author><name></name></author><category term="Liquidity" /><category term="Banking" /><category term="Michigan" /><summary type="html"><![CDATA[DISCLAIMER: This is a personal blog. Any views or opinions expressed in this blog do not represent the official policy or position of the Michigan Department of Insurance and Financial Services. Introduction This article investigates bank-specific factors that determine the level of liquid assets held by state chartered commercial banks in Michigan. Bank-specific factors include bank size, profitability, capital adequacy, deposits, and asset quality. This study consists of balanced panel data of 59 state chartered commercial banks from December 31st, 2019, until June 30th, 2023. Under the fractional reserve banking system, banks only hold a fraction of their deposit liabilities in liquid assets to meet anticipated liquidity needs. Bank management is responsible for determining the appropriate level of liquid assets required to meet the demands of depositors and creditors while allowing for the greatest profitability. Developing a better understanding of the determinants of liquid assets will give key insights into ongoing liquidity trends. Literature Review In 1999, an analysis of the Mexican banking system found that capital had a significant positive effect on liquid assets while total asset size (in relation to the total assets of the banking system) had no statistically significant effect on any measure of liquid assets. Additionally, this study theorized that banks with relatively more demand deposits possess more liquid assets but found that the level of demand deposits relative to total assets had a significant negative effect on securities and liquid assets but not the level of cash. This finding could suggest that banks consider demand deposits to be a very stable liability, or the variance of deposit withdraws declines as deposits increase and the deposit population becomes more diverse (Alger &amp; Alger, 1999). The relationship between liquid assets and bank size was further examined in a 2015 study of 18 Tunisian banks. This study used the natural logarithm of total assets as a measure of bank size and also found that bank size did not have a significant impact on bank liquidity. Additional variables in this study included return on assets (ROA), return on equity (ROE), net interest margin (NIM), total loans, operating expenses, total deposits, financial expenses, equity, inflation rate, and growth rate of GDP. ROA, ROE, capital, operating expenses, growth rate of GDP, and inflation rate were found to have had a significant impact on bank liquidity. Total loans, financial costs, and total deposits did not have a significant impact on bank liquidity (Moussa, 2015). Macroeconomic as well as bank-specific determinants of liquidity of Indian banks were studied by Singh and Sharma (2016). Bank-specific determinants studied in this paper include bank size, profitability, cost of funding, capital adequacy, and deposits. Macroeconomic determinants of liquidity included GDP, inflation, and unemployment. This study found that bank size and GDP had a negative relationship with liquidity at a 5 percent confidence interval while profitability, deposits, inflation, and capital adequacy had a positive impact. Specification of Variables The dependent variable in this article is liquidity which was measured through two liquidity ratios provided by Vodová (2011). Fixed and random effect regression was performed for both measures of liquidity. The first measure of liquidity is net loans as a fraction of total assets (Liq1). The ratio Liq1 indicates the percentage of bank assets which are invested in illiquid loans. The higher this ratio is, the fewer liquid assets a bank possesses. The second measure of liquidity is liquid assets as a fraction of total assets (Liq2). The ratio Liq2 indicates the percentage of bank assets which are liquid. A high ratio may indicate an inefficiency since liquid assets yield less income than illiquid assets. Liquid assets consist of interest as well as non-interest bearing bank balances, securities sold under agreement to repurchase, federal funds sold, and total securities less pledged securities. Table 1 Summary of variables and expected relationships with the dependent variables. Independent VariableProxy/MeasurementNotationExpected Effect Bank SizeNatural log of total assetsSIZE Negative ProfitabilityReturn on assetsROA Negative DepositsDeposits over total assetsDEP Positive Capital AdequacyTier 1 leverage ratioCAP Positive Interest MarginNet interest marginNIM Negative Asset QualityNonaccrual loans over total loansIMLOAN Negative Bank Size (SIZE). The size of banks (SIZE) has been measured using the natural logarithm of total assets. Sing et al. (2016) and El-Chaarani (2019) stated that bank size has a significant negative effect on liquidity. Smaller banks rely on a buffer of liquid assets while larger banks rely on credit instruments and the inter-bank market which is in accordance with the “too big to fail” hypothesis. Profitability (ROA). Return on assets is the proxy of profitability in this article. Return on Assets (TTM) measures the net income over the prior four quarters over average total assets. Moussa (2015) found that banks with higher return on assets had lower levels of liquidity. Banks with less liquid assets have a higher return on assets due to liquid assets generally yielding less income than illiquid assets. Deposits (DEP). Deposits (DEP) have been calculated as total deposits over total assets. Sing et al. (2016) stated that the level of deposits has a positive effect on liquidity. Deposits are the major source of funds for banks and these institutions must maintain adequate levels of liquidity to meet customer demand. Capital Adequacy (CAP). The tier 1 leverage ratio is the proxy of capital adequacy in this article. Sing et al. (2016) and Alger et al. (1999) found that capital adequacy had a positive influence on liquidity as a whole. Alger et al. (1999) suggests that banks invest more in liquid assets as a precaution when more capital is at stake. Interest Margin (NIM). Net interest margin is the measure of interest income less interest expenses over average earning assets. According to finance theory, interest margin should be negatively correlated with liquidity. Vodová (2013) found that liquidity is negatively affected by interest margin. Asset Quality (IMLOAN). Total nonaccrual loans over total loans is the proxy of asset quality in this article. Munteanu (2012) stated that the level of impaired loans has a significant negative effect on liquidity. A higher level of nonaccrual loans may lead to increased lending activity to make up for losses thus decreasing the level of liquid assets. Model Specification Two models will be estimated: \[Liq1_{it} = \alpha_{it}+β_{1}SIZE_{it}+β_{2}ROA_{it}+β_{3}DEP_{it}+β_{4}CAP_{it}+β_{5}IMLOAN_{it}+ ε_{it}\] \[Liq2_{it} = \alpha_{it}+β_{1}SIZE_{it}+β_{2}ROA_{it}+β_{3}DEP_{it}+β_{4}CAP_{it}+β_{5}IMLOAN_{it}+ ε_{it}\] β1, β2 , β3, β4, and β5 are the coefficients of the determinant variables and ε is the error term. The panel data was constructed with indices “i” and “t” representing individual banks and time, respectively. The data is comprised of 59 banks from December 31st, 2019, until June 30th, 2023. The total number of observations is 885. Descriptive Statistics Table 2 highlights the descriptive statistics for state chartered Michigan banks. Based on the findings below, net loans on average represented 59 percent of total assets (Liq1) for Michigan banks. Additionally, liquid assets constituted 33 percent of total assets (Liq2) on average. Table 2 Descriptive Statistics (Sample 12/31/2019 - 06/30/2023) Fixed Effect and Random Effect Regression Fixed effect and random effect regressions were run and a Hausman test was carried out to choose the appropriate model. The Hausman Test for both measures of liquidity resulted in a p-value less than 0.05, confirming that the fixed effect regression was the preferred model for Liq1 and Liq2. Results are shown in Table 3. The fixed effect models concluded that DEP, IMLOAN, CAP, NIM, and ROA significantly impacted the first measure of liquidity (Liq1) as well as the second measure (Liq2). The impact of DEP, IMLOAN, and ROA on Liq1 (net loans / total assets) was negative whereas the impact of CAP and NIM was positive. However, the impact of DEP, IMLOAN, and ROA on Liq2 (liquid assets / total assets) was positive while the impact of CAP and NIM was negative. SIZE was found to have an insignificant effect on either liquidity measure (Table 4). Table 3 Hausman Test ModelChi-Sq. StatisticProb. Liq1 Model33.9656.833e-06 Liq2 Model56.2742.562e-10 Table 4 Regression Analysis Conclusion The aim of this article was to identify bank-specific determinants of liquidity for state chartered Michigan Banks. Panel data regression analysis has been used for two measures of liquidity. Bank-specific determinants analyzed include bank size, total deposits, asset quality, capital adequacy, interest margin, and profitability. The findings of this study afford key insights into ongoing liquidity trends and risk. Liquidity is positively affected by the level of total deposits. In other words, the greater the level of total deposits, the higher the level of liquidity banks hold. Similar results were found in a study by Sing et al. (2016). However, a study by Alger et al. (1999) found a negative relationship between deposits and bank liquidity. Another major finding of this study is that the level of nonaccrual loans has a positive effect on liquidity. Thus, Michigan banks hold more liquid assets the higher the level of nonaccrual loans. This finding contradicts studies by Munteanu (2012) and El-Chaarani (2019) that have found that nonperforming loans have a negative impact on bank liquidity. Capital adequacy has a significant negative effect on liquidity. This study contradicts the findings of Sing et al. (2016) and Alger et al. (1999) but is in line with a study conducted by Moussa (2015). The reasons behind this discrepancy is a subject for future study. In line with the findings of Vodová (2013), net interest margin has a negative effect on bank liquidity. If net interest margin increases by 1 percent, liquidity declines by 8.25 percent and 8.45 percent in the Liq1 and Liq2 models, respectively. Vodová (2013) concluded that higher interest margin leads to increased lending activity thus reducing bank liquidity. The level of liquid assets held by Michigan banks is positively affected by bank profitability. Similar results were reported by Vodová (2013) and Sing et al. (2016). Higher profitability could indicate that more risk is present on the balance sheet which could require a higher liquidity buffer to mitigate risk. Bank size has no statistically significant relationship with the liquidity of Michigan state chartered banks. This may be due to the size of the financial institutions included in this article. Studies by Sing et al. (2016) and El-Chaarani (2019) found that smaller banks held a buffer of liquid assets while larger banks held less liquid assets and relied on credit instruments and the inter-bank market. The banks included in this study may be too small to rely on credit instruments and the inter-bank market to meet liquidity needs. References Alger, G., &amp; Alger, I. (1999). Liquid assets in banks: Theory and practice. GREMAQ, Universite des Sciences Sociales. El-Chaarani, H. (2019). Determinants of bank liquidity in the Middle East region. El-CHAARANI H.,(2019), Determinants of Bank Liquidity in the Middle East Region, International Review of Management and Marketing, 9(2). Moussa, M. A. B. (2015). The determinants of bank liquidity: Case of Tunisia. International journal of economics and financial issues, 5(1), 249-259. Munteanu, I. (2012). Bank liquidity and its determinants in Romania. Procedia Economics and Finance, 3, 993-998. Singh, A., &amp; Sharma, A. K. (2016). An empirical analysis of macroeconomic and bank-specific factors affecting liquidity of Indian banks. Future Business Journal, 2(1), 40-53. Vodová, P. (2011). Liquidity of Czech commercial banks and its determinants. International Journal of mathematical models and methods in applied sciences, 5(6), 1060-1067. Vodová, P. (2013). Determinants of commercial bank liquidity in Hungary. Finansowy Kwartalnik Internetowy e-Finanse, 9(3), 64-71.]]></summary></entry><entry><title type="html">Exploring scikit-learn</title><link href="https://danieljunghans.github.io/al-folio/blog/2020/SCIKITLearn/" rel="alternate" type="text/html" title="Exploring scikit-learn" /><published>2020-08-28T02:34:00+00:00</published><updated>2020-08-28T02:34:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2020/SCIKITLearn</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2020/SCIKITLearn/"><![CDATA[<p style="text-align: center;"><font size="+3">Introduction</font></p>
<p>For my current NEAT project, I read the paper <a href="https://ieeexplore.ieee.org/abstract/document/8473214?casa_token=mA1Va18Dm6kAAAAA:v_6_aQSag5JUXPvV3uPm-BYIVUfWLtCD5HZFDXopj5UUDriA460pLKGfCr99nKgQEYCw8a-GAQ"><em>A Comparative Study of Supervised Machine Learning Algorithms for Stock Market Trend Prediction</em></a> to learn  how others approached stock market prediction using supervised machine learning algorithms. This paper compared the accuracy between Support Vector Machine, Random Forest, K-Nearest Neighbor, Naive Bayes, and SoftMax algorithms. After reading about these different algorithms, I took it upon myself to learn more about the scikit-learn python library. I am learning how to use scikit-learn because it will give me the ability to implement some the algorithms outlined in the paper. The algorithms I have covered so far include: <br />
<a href="#RandomForest">•	Random Forest</a><br />
<a href="#KNN">•	K-Nearest Neighbors</a><br />
<a href="#SVM">•	Support Vector Machine</a><br /></p>

<p><a name="RandomForest"></a></p>
<p style="text-align: center;"><font size="+3">Random Forest</font></p>
<p>The first algorithm I sought to better understand was the Random Forest algorithm. This algorithm is made up of multiple decision trees. A decision tree learns simple decision rules from the data and produces a output. Each tree is used on a different sub sample of the dataset and averaging is used to improve accuracy. The decision trees all “vote” by producing outputs, and whatever the most popular output is becomes the algorithms output.</p>

<p>I first imported my data as a Pandas data frame and split it up into a training section and a testing section. I then created a Random Forest algorithm using the scikit-learn random forest classifier. After running the algorithm a few times, I noticed that the accuracy was unusually high. After some investigation, I discovered that the algorithms were predicting the stock price movement for the current day instead of one day in the future. After fixing the problem, the accuracy on the testing dataset stayed around 50%. Here is the code that I wrote:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="c1">#this opens the file and puts the data in a pandas dataframe
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">RandomForest.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>

<span class="c1">#setting the size of the training dataset
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">7</span>
<span class="n">split</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="p">.</span><span class="mi">7</span><span class="p">)</span>

<span class="c1">#Identifying all of the inputs and the expected output
</span><span class="n">X</span><span class="o">=</span><span class="n">data</span><span class="p">[[</span><span class="sh">'</span><span class="s">Open</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Low</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Close</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Accumulation Distribution Line</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">MACD</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Chaikan Oscillator (CHO)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Highest closing price (5 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Lowest closing price (days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Stochastic %K (5 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">%D</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume Price Trend (VPT)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Williams %R (14 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Relative Strength Index</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Momentum (10 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Price rate of change (PROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume rate of change (VROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">On Balance Volume (OBV)</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">Outputs</span><span class="sh">'</span><span class="p">]</span>

<span class="c1">#splitting the dataframe into a training and testing dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>

<span class="c1">#create a the random forest with 500 decision trees
</span><span class="n">clf</span><span class="o">=</span><span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span> 

<span class="c1">#train the model on the training data
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1">#making predictions on the testing data
</span><span class="n">y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy:</span><span class="sh">"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p><a name="KNN"></a></p>
<p style="text-align: center;"><font size="+3">K-Nearest Neighbors</font></p>
<p>The next algorithm I studied was the K-Nearest Neighbors algorithm. The nearest neighbor method looks for the most similar trading days. The predefined number of the most similar trading days become the nearest neighbors. Just like the decision trees in the Random Forest algorithm, the nearest neighbors “vote”. Instead of using decision trees to produce an output, the K-Nearest Neighbor algorithm uses the actual expected output for the most similar trading days. The most popular output becomes of the neighbors becomes the algorithms output. If 5 of the nearest neighbors had the closing stock price go up and 3 neighbors had the stock price go down, the prediction will be that the stock price goes up.</p>

<p>While studying this algorithm, I learned that K-Nearest Neighbor algorithms performs better with a lower number of features/inputs. To reduce the size of my dataset, I first normalized it with the standard scaler tool from scikit-learn. I then used PCA to shrink the number of inputs in my dataset down to two while preserving the variance in the dataset.  Here is my K-Nearest Neighbors code:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1">#this opens the CSV File
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">RandomForest.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>

<span class="c1">#preprocessing and normalizing
</span><span class="n">Features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Open</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Low</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Close</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Accumulation Distribution Line</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">MACD</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Chaikan Oscillator (CHO)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Highest closing price (5 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Lowest closing price (days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Stochastic %K (5 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">%D</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume Price Trend (VPT)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Williams %R (14 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Relative Strength Index</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Momentum (10 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Price rate of change (PROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume rate of change (VROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">On Balance Volume (OBV)</span><span class="sh">'</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">Features</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="sh">'</span><span class="s">Outputs</span><span class="sh">'</span><span class="p">]].</span><span class="n">values</span>

<span class="c1">#normalizing the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#performing PCA
</span><span class="n">PCA</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Components</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ComponentDf</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Components</span><span class="p">)</span>

<span class="c1">#setting the size of the training set
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">8</span>
<span class="n">split</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="n">training_size</span><span class="p">)</span>

<span class="c1">#splitting up the dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ComponentDf</span><span class="p">[:</span><span class="n">split</span><span class="p">])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ComponentDf</span><span class="p">[</span><span class="n">split</span><span class="p">:])</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Y</span><span class="p">[:</span><span class="n">split</span><span class="p">])</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Y</span><span class="p">[</span><span class="n">split</span><span class="p">:])</span>

<span class="c1">#create the K Nearest Neighbor Algorithm and running the algorithm
</span><span class="n">clf</span><span class="o">=</span><span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>
<span class="n">Y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy 
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy:</span><span class="sh">"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>After performing principal component analysis on my dataset, I graphed the principal components to better understand the data. After creating the graph, it became clear why the accuracy of my K-Nearest Neighbor algorithm stayed around 50%. Both expected outputs (Stock price going up or down) are clustered together.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/Component.PNG" />
</div>

<p><a name="SVM"></a></p>
<p style="text-align: center;"><font size="+3">Support Vector Machine</font></p>
<p>A Support Vector Machine is a machine learning tool that uses a hyperplane to define data points. Instead of looking at the nearest neighbors like a KNN, I like to think that Support Vector Machines split up data points into different neighborhoods. For example, if we use the two components from my PCA analysis, a SVM will create a one dimensional hyperplane splitting up the data into two “neighborhoods”. The hyperplane will create decision boundaries that maximize the margins from both expected outputs. The graph below shows the data points and the decision boundaries created by my SVM.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph10.png" />
</div>

<p>Any data point that falls into the blue background gets categorized as a 0 (closing stock price goes down). To create this graph, I used a Support Vector Machine with a RBF kernel. Kernel functions allow SVMs to create hyperplanes in high dimensional data without having to calculate the coordinates of the data in that space. Here is my Support Vector Machine Code:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
</pre></td><td class="code"><pre><span class="c1">#support vector machine
</span>
<span class="kn">import</span> <span class="n">csv</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#this opens the CSV File
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">RandomForest.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>

<span class="c1">#setting the size of the training dataset
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">7</span>
<span class="n">split</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="p">.</span><span class="mi">7</span><span class="p">)</span>

<span class="c1">#Identifying all of the inputs and the expected output
</span><span class="n">X</span><span class="o">=</span><span class="n">data</span><span class="p">[[</span><span class="sh">'</span><span class="s">Open</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Low</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Close</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Accumulation Distribution Line</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">MACD</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Chaikan Oscillator (CHO)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Highest closing price (5 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Lowest closing price (days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Stochastic %K (5 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">%D</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume Price Trend (VPT)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Williams %R (14 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Relative Strength Index</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Momentum (10 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Price rate of change (PROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume rate of change (VROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">On Balance Volume (OBV)</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">Outputs</span><span class="sh">'</span><span class="p">]</span>

<span class="c1">#splitting the dataframe into a training and testing dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>

<span class="c1">#create the support vector machine
</span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">svc</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">svc</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy:</span><span class="sh">"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1">##################################################
#The next section transforms the dataset#
#And graphs the components with decision boundries
</span>
<span class="c1">#Normalizing the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#performing PCA
</span><span class="n">PCA</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Components</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ComponentDf</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Components</span><span class="p">,</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">principal component 1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">principal component 2</span><span class="sh">'</span><span class="p">])</span>

<span class="c1">#transforming the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">ComponentDf</span><span class="p">.</span><span class="nf">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>

<span class="n">h</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># create a mesh to plot in
</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span>
                     <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>

<span class="c1">#create the support vector machine
</span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1">#graphing the data
</span><span class="n">Z</span> <span class="o">=</span> <span class="n">svc</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()])</span>

<span class="c1"># Put the result into a color plot
</span><span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Creating the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">xx</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">yy</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Support Vector Machine Graph</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="c1"># Saving the Plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">matplotlib.png</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="Machine-Learning" /><summary type="html"><![CDATA[Introduction For my current NEAT project, I read the paper A Comparative Study of Supervised Machine Learning Algorithms for Stock Market Trend Prediction to learn how others approached stock market prediction using supervised machine learning algorithms. This paper compared the accuracy between Support Vector Machine, Random Forest, K-Nearest Neighbor, Naive Bayes, and SoftMax algorithms. After reading about these different algorithms, I took it upon myself to learn more about the scikit-learn python library. I am learning how to use scikit-learn because it will give me the ability to implement some the algorithms outlined in the paper. The algorithms I have covered so far include: • Random Forest • K-Nearest Neighbors • Support Vector Machine Random Forest The first algorithm I sought to better understand was the Random Forest algorithm. This algorithm is made up of multiple decision trees. A decision tree learns simple decision rules from the data and produces a output. Each tree is used on a different sub sample of the dataset and averaging is used to improve accuracy. The decision trees all “vote” by producing outputs, and whatever the most popular output is becomes the algorithms output. I first imported my data as a Pandas data frame and split it up into a training section and a testing section. I then created a Random Forest algorithm using the scikit-learn random forest classifier. After running the algorithm a few times, I noticed that the accuracy was unusually high. After some investigation, I discovered that the algorithms were predicting the stock price movement for the current day instead of one day in the future. After fixing the problem, the accuracy on the testing dataset stayed around 50%. Here is the code that I wrote: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import csv import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn import metrics from sklearn.metrics import classification_report #this opens the file and puts the data in a pandas dataframe data = pd.read_csv('RandomForest.csv') data.head() #setting the size of the training dataset training_size = .7 split = int(len(data)*.7) #Identifying all of the inputs and the expected output X=data[['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)']] y=data['Outputs'] #splitting the dataframe into a training and testing dataset X_train = X[:split] X_test = X[split:] y_train = y[:split] y_test = y[split:] #create a the random forest with 500 decision trees clf=RandomForestClassifier(n_estimators=500) #train the model on the training data clf.fit(X_train,y_train) #making predictions on the testing data y_pred=clf.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) K-Nearest Neighbors The next algorithm I studied was the K-Nearest Neighbors algorithm. The nearest neighbor method looks for the most similar trading days. The predefined number of the most similar trading days become the nearest neighbors. Just like the decision trees in the Random Forest algorithm, the nearest neighbors “vote”. Instead of using decision trees to produce an output, the K-Nearest Neighbor algorithm uses the actual expected output for the most similar trading days. The most popular output becomes of the neighbors becomes the algorithms output. If 5 of the nearest neighbors had the closing stock price go up and 3 neighbors had the stock price go down, the prediction will be that the stock price goes up. While studying this algorithm, I learned that K-Nearest Neighbor algorithms performs better with a lower number of features/inputs. To reduce the size of my dataset, I first normalized it with the standard scaler tool from scikit-learn. I then used PCA to shrink the number of inputs in my dataset down to two while preserving the variance in the dataset. Here is my K-Nearest Neighbors code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import csv import pandas as pd from sklearn import metrics from sklearn.metrics import classification_report from sklearn.neighbors import KNeighborsClassifier from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler #this opens the CSV File data = pd.read_csv('RandomForest.csv') data.head() #preprocessing and normalizing Features = ['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)'] X = data.loc[:, Features].values Y = data.loc[:,['Outputs']].values #normalizing the dataset X = StandardScaler().fit_transform(X) #performing PCA PCA = PCA(n_components=2) Components = PCA.fit_transform(X) ComponentDf = pd.DataFrame(data=Components) #setting the size of the training set training_size = .8 split = int(len(data)*training_size) #splitting up the dataset X_train = pd.DataFrame(data=ComponentDf[:split]) X_test = pd.DataFrame(data=ComponentDf[split:]) Y_train = pd.DataFrame(data=Y[:split]) Y_test = pd.DataFrame(data=Y[split:]) #create the K Nearest Neighbor Algorithm and running the algorithm clf=KNeighborsClassifier(n_neighbors=5) clf.fit(X_train, Y_train.values.ravel()) Y_pred=clf.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(Y_test, Y_pred)) After performing principal component analysis on my dataset, I graphed the principal components to better understand the data. After creating the graph, it became clear why the accuracy of my K-Nearest Neighbor algorithm stayed around 50%. Both expected outputs (Stock price going up or down) are clustered together. Support Vector Machine A Support Vector Machine is a machine learning tool that uses a hyperplane to define data points. Instead of looking at the nearest neighbors like a KNN, I like to think that Support Vector Machines split up data points into different neighborhoods. For example, if we use the two components from my PCA analysis, a SVM will create a one dimensional hyperplane splitting up the data into two “neighborhoods”. The hyperplane will create decision boundaries that maximize the margins from both expected outputs. The graph below shows the data points and the decision boundaries created by my SVM. Any data point that falls into the blue background gets categorized as a 0 (closing stock price goes down). To create this graph, I used a Support Vector Machine with a RBF kernel. Kernel functions allow SVMs to create hyperplanes in high dimensional data without having to calculate the coordinates of the data in that space. Here is my Support Vector Machine Code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 #support vector machine import csv import pandas as pd from sklearn import metrics from sklearn.metrics import classification_report from sklearn import svm from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt import numpy as np #this opens the CSV File data = pd.read_csv('RandomForest.csv') data.head() #setting the size of the training dataset training_size = .7 split = int(len(data)*.7) #Identifying all of the inputs and the expected output X=data[['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)']] y=data['Outputs'] #splitting the dataframe into a training and testing dataset X_train = X[:split] X_test = X[split:] y_train = y[:split] y_test = y[split:] #create the support vector machine svc = svm.SVC(kernel='rbf', C=1.0) svc.fit(X_train, y_train) y_pred = svc.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) ################################################## #The next section transforms the dataset# #And graphs the components with decision boundries #Normalizing the dataset X = StandardScaler().fit_transform(X) #performing PCA PCA = PCA(n_components=2) Components = PCA.fit_transform(X) ComponentDf = pd.DataFrame(data=Components,columns = ['principal component 1', 'principal component 2']) #transforming the dataset X = ComponentDf.to_numpy() y = y.ravel() h = 0.2 # create a mesh to plot in x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) #create the support vector machine svc = svm.SVC(kernel='rbf', C=1.0).fit(X, y) #graphing the data Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8) # Creating the plot plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm) plt.xlabel('Principal Component 1') plt.ylabel('Principal Component 2') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xticks(()) plt.yticks(()) plt.title('Support Vector Machine Graph') plt.legend() # Saving the Plot plt.savefig("matplotlib.png")]]></summary></entry><entry><title type="html">2020 Summer Reflection</title><link href="https://danieljunghans.github.io/al-folio/blog/2020/Reflection/" rel="alternate" type="text/html" title="2020 Summer Reflection" /><published>2020-08-20T01:44:00+00:00</published><updated>2020-08-20T01:44:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2020/Reflection</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2020/Reflection/"><![CDATA[<p style="text-align: center;"><font size="+3">Reflection</font></p>
<p>Looking back, I am very satisfied with the work I have done this summer. I started off the summer by working on my NEAT project and taking the course Financial Management 311. The beginning of this course focused on how to construct an income statement, balance sheet, and statement of cash flows. The next big concept this course covered was the time value of money and compound interest. I learned about the time value of money by doing present and future value calculations in excel. The last section of this course focused on bonds and capital budgeting decisions. I first learned about bonds and bond ratings, but quickly began performing NPV, payback period, discounted payback period, and IRR calculations.</p>

<p><br />
Financial Management 311 was very informative and a fun summer class. This course greatly expanded my knowledge of financial instruments and capital budgeting decisions. Although this class consumed a lot of my time, I was still working on other things. I continued my NEAT project during this time and was able to finish my website.</p>

<p><br /> 
After finishing Financial Management 311, I started taking Principles of Management Accounting 202. This course started by helping me understand the differences between financial and management accounting. Next, I began to learn about different cost classifications, and the basic manufacturing cost categories. With this new knowledge of cost classifications, I started to create traditional and contribution format income statements. Moving on from contribution income statements, I learned how to calculate the predetermined overhead rate and the different ways overhead is allocated. After learning about manufacturing overhead, I began to learn about costing methods and the flow of costs from raw materials to the cost of goods manufactured. Finally, the last few chapters for this course focused on Cost Volume Profit analysis, break even analysis, ROI, and Residual Income calculations.</p>

<p style="text-align: center;"><font size="+3">Summer Research Update</font></p>
<p>Throughout the summer, I continued to work on my NEAT project while taking the summer courses described above. After reading several papers about algorithms predicting stock price movement, I decided to change my algorithm so it is predicting whether the closing stock price will go up or down. Previously my algorithm predicting the closing price for the next day, but now it just predicts whether the price is going up or down. After changing the expected outputs, here was the algorithm accuracy for all models:</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph9.PNG" />
</div>

<p>After running NEAT I created a random forrest algorithm using the python library <a href="https://scikit-learn.org/stable/">scikit learn</a>. The new random forrest algorithm performed very similiarly to my NEAT. For the next few weeks, I plan on becoming more familiar with the scikit learn library.</p>]]></content><author><name></name></author><category term="Updates" /><summary type="html"><![CDATA[Reflection Looking back, I am very satisfied with the work I have done this summer. I started off the summer by working on my NEAT project and taking the course Financial Management 311. The beginning of this course focused on how to construct an income statement, balance sheet, and statement of cash flows. The next big concept this course covered was the time value of money and compound interest. I learned about the time value of money by doing present and future value calculations in excel. The last section of this course focused on bonds and capital budgeting decisions. I first learned about bonds and bond ratings, but quickly began performing NPV, payback period, discounted payback period, and IRR calculations. Financial Management 311 was very informative and a fun summer class. This course greatly expanded my knowledge of financial instruments and capital budgeting decisions. Although this class consumed a lot of my time, I was still working on other things. I continued my NEAT project during this time and was able to finish my website. After finishing Financial Management 311, I started taking Principles of Management Accounting 202. This course started by helping me understand the differences between financial and management accounting. Next, I began to learn about different cost classifications, and the basic manufacturing cost categories. With this new knowledge of cost classifications, I started to create traditional and contribution format income statements. Moving on from contribution income statements, I learned how to calculate the predetermined overhead rate and the different ways overhead is allocated. After learning about manufacturing overhead, I began to learn about costing methods and the flow of costs from raw materials to the cost of goods manufactured. Finally, the last few chapters for this course focused on Cost Volume Profit analysis, break even analysis, ROI, and Residual Income calculations. Summer Research Update Throughout the summer, I continued to work on my NEAT project while taking the summer courses described above. After reading several papers about algorithms predicting stock price movement, I decided to change my algorithm so it is predicting whether the closing stock price will go up or down. Previously my algorithm predicting the closing price for the next day, but now it just predicts whether the price is going up or down. After changing the expected outputs, here was the algorithm accuracy for all models: After running NEAT I created a random forrest algorithm using the python library scikit learn. The new random forrest algorithm performed very similiarly to my NEAT. For the next few weeks, I plan on becoming more familiar with the scikit learn library.]]></summary></entry><entry><title type="html">NEAT Project</title><link href="https://danieljunghans.github.io/al-folio/blog/2020/NEAT/" rel="alternate" type="text/html" title="NEAT Project" /><published>2020-06-18T06:13:00+00:00</published><updated>2020-06-18T06:13:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2020/NEAT</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2020/NEAT/"><![CDATA[<p style="text-align: center;"><font size="+3">Introduction</font></p>
<p>I began my current research project with a simple question: can I evolve an artificial neural network (ANN) to accurately predict the closing price of a stock?</p>

<p style="text-align: center;"><font size="+3">Artificial Neural Networks</font></p>
<p>ANNs are inspired by biological brains and are made up of densely interconnected nodes. Instead of neurons and synapses, ANNs utilize artificial neurons (nodes) and connections.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/neuralnetwork.png" />
</div>

<p style="text-align: center;"><i><strong>Figure 1</strong> Visual representation of an artificial neural network. The circles represent nodes and the lines connecting the nodes are connections. Nodes change their outputs based on the values received from connections.  </i></p>

<p><br /></p>

<h4>Input Layer</h4>
<p>All ANNs are made up of 3 types of layers. The first layer is the input layer shown in Figure 1. This is the initial data that is read by the neural networks. For this project, the input layer will consist of historical trading data. To learn about the inputs I am using, scroll down to the <a href="#stock">Stock Data section</a>.</p>

<h4>Connections</h4>
<p>Connections transmit signals in the form of real numbers and each connection has its own weight. The purpose of connection weights is to either strengthen or weaken the signal that is being transmitted. ANNs learn by adjusting their connection weights. As ANNs learn, important connections will develop larger weights as weights for unimportant connections become smaller.</p>

<h4>Hidden Layers</h4>
<p>The hidden layers come after the input layer and is where all the computation is done. The artificial neural network in Figure 1 has two hidden layers made up of four nodes in each layer. The nodes that are in the hidden layers must find the sum of all input signals multiplied by their corresponding weights, add a node bias, and plug the result in an Activation Function.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/node.png" />
</div>

<p style="text-align: center;"><i><strong>Figure 2</strong> visual representation of a node. The large circle represents the node and shows the addition of the bias to the summation of the inputs multiplied by the connection weights. The value the node calculates is then plugged into a non-linear activation function and the result is the node output. </i></p>

<p><br />
An Activation Function is a non-linear function used by nodes to produce an output. A common example of an Activation Function would be the sine function. The node bias shown in Figure 2 is a value added to the sum of all input signals multiplied by their weights. The purpose of adding a node bias is to move the Activation Function. The bias acts very similarly to the constant b in the linear function: <em>y=ax+b</em>. When the constant <em>b</em> is changed, the line will move up or down changing the <em>y-axis</em>. Moving an Activation Function greatly impacts the output of a node which may be useful in the learning process.</p>

<h4>Output Layer</h4>
<p>The last layer for Figure 1 is the output layer. The output layer is that last set of nodes that produce outputs. My ANNs will only have one node in the output layer. This last node will be responsible for outputting the closing price prediction.</p>

<p style="text-align: center;"><font size="+3">NEAT</font></p>
<p>While researching ANNs, I was introduced to the <a href="#references">Neural Evolution of Augmenting Topologies</a> (NEAT). NEAT was developed by Dr. Kenneth O. Stanely and is a method for evolving the structure ANNs. Unlike traditional ANNs that only adjust connection weights, NEAT gives ANNS the ability to change nodes, weights, and connections. The NEAT approach allows ANNS to add or delete connections and nodes. Stock market data is extremely noisy and allowing the structure of my ANNs to change may give them a competitive advantage.</p>

<p><a name="stock"></a></p>
<p style="text-align: center;"><font size="+3">Tesla Stock</font></p>
<p>For this project, I am using a stock with a highly volatile price. I want to use a stock with a highly volatile price because I believe it will lead to more versatile and complex algorithms. I chose to train on <a href="https://finance.yahoo.com/quote/TSLA?p=TSLA&amp;.tsrc=fin-srch">$TSLA</a> because it meets my volatility requirement with a 52-week price range of $211.00 - $1027.48.</p>

<p style="text-align: center;"><font size="+3">Stock Data</font></p>
<p>The financial data consists of historical trading data and technical indicators. There are 19 variables in my dataset which serve as inputs for my ANNs. The financial data I decided to use for this research project was largely influenced by <a href="#references"><em>Improving Stock Closing Price Prediction Using Recurrent Neural Network and Technical Indicators</em></a>. The historical trading data was downloaded from yahoo finance.</p>

<p style="text-align: center;"><font size="+3">Neat-Python</font></p>

<p>I used the <a href="https://neat-python.readthedocs.io/en/latest/">neat-python</a> implementation of NEAT for this project. Neat-python is a library with tools that maintain individual genomes (artificial neural networks). These genomes are made up of genes that contain important information about the structure of the algorithm.</p>

<p style="text-align: center;"><font size="+3">Methods</font></p>

<h4>Training and Testing Data</h4>
<p>The first thing my program does is split my financial dataset into two pieces. The first 70% of the dataset becomes the training data, and the last 30% of the dataset becomes the testing data. <br /></p>

<h4>Measuring Performance</h4>
<p>The training dataset is first randomly sampled. Then the ANNs produce outputs (closing price predictions) from the sampled data. While the ANNs produce outputs, their error
is determined by subtracting their output from the expected output (closing stock price). 
<br /></p>
<h4>Speciation and Reproduction</h4>
<p>Once the ANNs have completed the training dataset, they are divided up into species based on genomic distance (this is a measure of how similar genomes are based on their structure i.e. nodes and connections). After all the ANNs have been put in a species, parents are selected to produce offspring through sexual and asexual reproduction. These offspring may be mutated and will be the next generation of ANNs. This cycle of measuring performance, speciation, selection, reproduction, and mutation will continue for <em>N</em> generations. 
<br /></p>
<h4>Testing Data</h4>
<p>After <em>N</em> generations, my code checks to see if the best ANN meets my performance threshold.</p>

<p>•	If the best performing ANN does not meet this threshold, the ANNs continue to run on the training data for <em>N</em> generations.</p>

<p>•	When the best ANN meets this performance threshold, it is run on the entire testing dataset. The outputs (stock price predictions) are recorded, and my code checks to see if the best neural network is good enough to stop running the program.</p>

<p style="text-align: center;"><font size="+3">Exploratory Runs</font></p>
<p>After I wrote the initial code, I ran several exploratory runs to make sure everything worked properly. These exploratory runs also helped me understand how NEAT behaves in different conditions. Before using financial data, I used numbers from a sine wave as inputs to see if my ANNs could predict the sine wave. The ANNs were successful and I made several adjustments to my code. One of these adjustments was lowering the species compatibility threshold because I noticed that most species collapsed after a few hundred generations.</p>

<p style="text-align: center;"><font size="+3">Current Experiments</font></p>
<p>My first experiment was to see if my ANNs could predict the closing stock price using normalized data. I graphed the best ANNs testing data predictions for all 50 runs. As a baseline, I chose to use yesterday’s closing price as the current day’s prediction. The baseline will be represented with red dots in Figures 3 through 5.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph1.PNG" />
</div>
<p style="text-align: center;"><i><strong>Figure 3</strong> Artificial neural network testing data results after 15,000 generations for all 50 runs.</i></p>
<p><br /></p>

<p>When looking at Figure 3, it is clear the ANNs could not perform as well as the baseline. I started to investigate what could cause this problem and came up with several hypotheses. My first hypothesis was that only giving the ANNs one Activation Function hindered their ability to predict large numbers. After giving the ANNs access to all Activation Functions in the neat-python library, this was the result:</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph2.PNG" />
</div>
<p style="text-align: center;"><i><strong>Figure 4</strong> Artificial neural network testing data results after 20,000 generations for all 50 runs. </i></p>
<p><br /></p>

<p>compared to the previous experiment, the ANNs clearly performed much better with access to all Activation Functions. Despite this improvement, the ANNs still failed to outperform the baseline as shown in Figure 4. My second hypothesis was that normalizing the data made it difficult for the ANNs to identify important inputs. Here is the result of giving the ANNs access to all Activation Functions and running them on un-normalized data:</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph3.PNG" />
</div>
<p style="text-align: center;"><i><strong>Figure 5</strong> Artificial neural network testing data results after 15000 generations for all 50 runs. The horizontal line added to the residual plot shows the average error for my baseline</i></p>
<p><br /></p>

<p>When looking at Figure 5, it is now obvious that my ANNs are doing as well as the baseline. Although the baseline is being met, the ANNs are using the baseline strategy and struggling to improve.</p>

<p style="text-align: center;"><font size="+3">Future Work</font></p>
<p>This project has come quite far, and I have plenty of ideas for the direction of this research project. For future experiments, I plan on experimenting with other selection schemes. I would also like to switch from feed forward to recurrent neural networks.</p>

<p style="text-align: center;"><font size="+3">References</font></p>
<p>Stanley, K. O., &amp; Miikkulainen, R. (2002, May). Efficient evolution of neural network topologies. In Proceedings of the 2002 Congress on Evolutionary Computation. CEC’02 (Cat. No. 02TH8600) (Vol. 2, pp. 1757-1762). IEEE.
<br /></p>

<p>Gao, T., &amp; Chai, Y. (2018). Improving stock closing price prediction using recurrent neural network and technical indicators. Neural computation, 30(10), 2833-2854.</p>

<p><a name="references"></a></p>]]></content><author><name></name></author><category term="Machine-Learning" /><category term="Finance" /><summary type="html"><![CDATA[Introduction I began my current research project with a simple question: can I evolve an artificial neural network (ANN) to accurately predict the closing price of a stock?]]></summary></entry></feed>